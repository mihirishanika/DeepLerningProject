{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11456205,"sourceType":"datasetVersion","datasetId":7178184}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split, KFold\nimport pickle\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Kaggle dataset paths\nBASE_DIR = '../input/deeplearning/'\nIMAGES_DIR = os.path.join(BASE_DIR, 'Images/Images/')\nCAPTIONS_PATH = os.path.join(BASE_DIR, 'captions.txt')\n\n# Create output directories for models and data\nos.makedirs('model', exist_ok=True)\nos.makedirs('data', exist_ok=True)\n\n# Load captions\nprint(\"Loading captions...\")\nwith open(CAPTIONS_PATH, \"r\") as f:\n    captions_data = f.readlines()\n\n# Process captions\ncleaned_captions = []\nimage_names = []\n\nfor line in captions_data:\n    line = line.strip()\n    if line:  # Skip empty lines\n        parts = line.split(',', 1)  # Split only at the first comma\n        if len(parts) >= 2:\n            image_name = parts[0].strip()\n            caption = parts[1].strip()\n            \n            # Add start and end tokens\n            caption = '<start> ' + caption + ' <end>'\n            \n            image_names.append(image_name)\n            cleaned_captions.append(caption)\n\nprint(f\"Loaded {len(cleaned_captions)} captions\")\nprint(f\"Sample caption: {cleaned_captions[0]}\")\nprint(f\"Sample image name: {image_names[0]}\")\n\n# Configure parameters\nvocab_size = 5000  # Define vocabulary size\nmax_length = 30    # Max caption length\nembedding_dim = 256\nunits = 256\nbatch_size = 32\nfeatures_shape = 4096  # VGG16 FC layer output shape\ndropout_rate = 0.5\n\n# Function to extract features from images using VGG16\ndef extract_features():\n    # Load VGG16 model\n    base_model = VGG16(weights='imagenet')\n    model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n    \n    features = {}\n    # Get list of all image files\n    image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.jpg')]\n    \n    print(f\"Extracting features from {len(image_files)} images...\")\n    \n    # Process images in batches to avoid memory issues\n    for img_name in tqdm(image_files):\n        img_path = os.path.join(IMAGES_DIR, img_name)\n        try:\n            img = Image.open(img_path)\n            img = img.resize((224, 224))\n            img = np.array(img)\n            \n            # Handle grayscale images\n            if len(img.shape) == 2:\n                img = np.stack((img,)*3, axis=-1)\n            elif img.shape[2] == 1:\n                img = np.concatenate([img, img, img], axis=2)\n            elif img.shape[2] == 4:  # Handle RGBA\n                img = img[:,:,:3]\n                \n            # Preprocess for VGG16\n            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n            img = tf.keras.applications.vgg16.preprocess_input(img)\n            \n            # Extract features\n            feature = model.predict(img, verbose=0)\n            features[img_name] = feature.flatten()\n        except Exception as e:\n            print(f\"Error processing {img_name}: {e}\")\n    \n    return features\n\n# Extract features from all images\nprint(\"Extracting features from images...\")\nimage_features = extract_features()\nprint(f\"Features extracted for {len(image_features)} images\")\n\n# Save the features\nwith open('data/image_features.pkl', 'wb') as f:\n    pickle.dump(image_features, f)\n\n# Tokenize captions\nprint(\"Tokenizing captions...\")\ntokenizer = Tokenizer(num_words=vocab_size, oov_token='<unk>')\ntokenizer.fit_on_texts(cleaned_captions)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\n# Save the tokenizer\nwith open('data/tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\n\n# Create sequences\ncaptions_sequences = tokenizer.texts_to_sequences(cleaned_captions)\ncaptions_padded = pad_sequences(captions_sequences, maxlen=max_length, padding='post')\n\n# Prepare training data\nX_data = []  # Features\ny_data = []  # Target captions\n\nfor i, img_name in enumerate(image_names):\n    if img_name in image_features:\n        X_data.append(image_features[img_name])\n        y_data.append(captions_padded[i])\n\nX_data = np.array(X_data)\ny_data = np.array(y_data)\n\nprint(f\"Training data shape: {X_data.shape}\")\nprint(f\"Target data shape: {y_data.shape}\")\n\n# Split dataset\nX_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:43:22.469999Z","iopub.execute_input":"2025-04-20T08:43:22.470253Z"}},"outputs":[{"name":"stderr","text":"2025-04-20 08:43:24.285077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745138604.514149      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745138604.583706      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading captions...\nLoaded 40456 captions\nSample caption: <start> caption <end>\nSample image name: image\nExtracting features from images...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745138618.189967      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745138618.190678      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\nExtracting features from 8090 images...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/8090 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745138625.038853      95 service.cc:148] XLA service 0x7d81d0004a30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745138625.039771      95 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745138625.039793      95 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745138625.188466      95 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1745138627.765589      95 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n 74%|███████▍  | 5999/8090 [08:46<03:01, 11.53it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Define CNN-LSTM Model for sequence prediction\ndef build_model(vocab_size, max_length, dropout=0.5, lstm_units=256):\n    # Image feature input\n    image_input = Input(shape=(features_shape,))\n    img_dense = Dense(embedding_dim, activation=\"relu\")(image_input)\n    img_dropout = Dropout(0.3)(img_dense)\n    img_repeat = tf.keras.layers.RepeatVector(max_length)(img_dropout)\n    \n    # Text sequence input\n    text_input = Input(shape=(max_length,))\n    text_embed = Embedding(vocab_size, embedding_dim, mask_zero=True)(text_input)\n    text_dropout = Dropout(0.3)(text_embed)\n    \n    # Merge image and text features\n    decoder_input = tf.keras.layers.Concatenate()([img_repeat, text_dropout])\n    \n    # LSTM decoder\n    decoder_lstm = LSTM(lstm_units, return_sequences=True)(decoder_input)\n    decoder_dropout = Dropout(dropout)(decoder_lstm)\n    output = Dense(vocab_size, activation=\"softmax\")(decoder_dropout)\n    \n    model = Model(inputs=[image_input, text_input], outputs=output)\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\n\n# Train Model with K-Fold Cross Validation\nprint(\"Training model with K-Fold cross validation...\")\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nbest_loss = float(\"inf\")\nfold = 1\n\nfor train_idx, val_idx in kf.split(X_train):\n    print(f\"\\nTraining Fold {fold}/5\")\n    fold += 1\n    \n    # Build model\n    model = build_model(vocab_size, max_length)\n    \n    # Prepare input sequences\n    train_img_features = X_train[train_idx]\n    train_captions = y_train[train_idx]\n    val_img_features = X_train[val_idx]\n    val_captions = y_train[val_idx]\n    \n    # Create target sequences (one word shifted)\n    train_targets = np.zeros_like(train_captions)\n    val_targets = np.zeros_like(val_captions)\n    \n    # Shift sequences by one position\n    for i in range(train_captions.shape[0]):\n        train_targets[i, :-1] = train_captions[i, 1:]\n    \n    for i in range(val_captions.shape[0]):\n        val_targets[i, :-1] = val_captions[i, 1:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n    \n    # Reshape targets to match what sparse_categorical_crossentropy expects\n    # We need to add a new dimension at the end\n    train_targets = train_targets.reshape(train_targets.shape[0], train_targets.shape[1], 1)\n    val_targets = val_targets.reshape(val_targets.shape[0], val_targets.shape[1], 1)\n    \n    # Train the model\n    history = model.fit(\n        [train_img_features, train_captions],\n        train_targets,\n        validation_data=([val_img_features, val_captions], val_targets),\n        epochs=10, batch_size=batch_size, verbose=1\n    )\n    \n    # Check if this model is better\n    val_loss = min(history.history['val_loss'])\n    if val_loss < best_loss:\n        best_loss = val_loss\n        model.save(\"model/best_caption_generator.h5\")\n        print(f\"New best model saved with validation loss: {best_loss:.4f}\")\n\nprint(\"Training complete. Best model saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}