{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import os\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# Extract ZIP file\n",
        "if not os.path.exists(\"temp_images\"):\n",
        "    with zipfile.ZipFile(\"Images.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"temp_images\")\n",
        "\n",
        "# captions_path is no longer needed, reading directly from captions.txt\n",
        "with open(\"captions.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    captions = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Define constants\n",
        "vocab_size = 5000  # Define vocabulary size\n",
        "max_length = 30  # Max caption length\n",
        "\n",
        "# Extract features using VGG16\n",
        "def extract_features_from_image(image_path):\n",
        "    vgg = VGG16(weights='imagenet', include_top=False)\n",
        "    model = Model(inputs=vgg.input, outputs=vgg.output)\n",
        "\n",
        "    img = Image.open(image_path).convert('RGB').resize((224, 224))\n",
        "    img = np.array(img)\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    img = preprocess_input(img)  # Preprocess for VGG16\n",
        "\n",
        "    features = model.predict(img, verbose=0)\n",
        "    return features.flatten()\n",
        "\n",
        "image_features = []\n",
        "image_filenames = []\n",
        "\n",
        "# Path to the subfolder containing images\n",
        "images_folder = os.path.join(\"temp_images\", \"Images\")\n",
        "\n",
        "# Check if 'temp_images' directory contains files\n",
        "if not os.listdir(\"temp_images\"):\n",
        "    raise ValueError(\"The 'temp_images' directory is empty. Please ensure it contains image files.\")\n",
        "\n",
        "images_folder = os.path.join(\"temp_images\", \"Images\")  # Correct subfolder\n",
        "for filename in os.listdir(images_folder):\n",
        "    file_path = os.path.join(images_folder, filename)\n",
        "\n",
        "    if os.path.isfile(file_path) and filename.lower().endswith((\".jpg\")):\n",
        "        try:\n",
        "            features = extract_features_from_image(file_path)\n",
        "            image_features.append(features)\n",
        "            image_filenames.append(filename)\n",
        "            print(f\"Processed: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "if not image_features:\n",
        "    raise ValueError(\"No image features extracted. Check if images exist in 'temp_images'.\")\n",
        "\n",
        "image_features = np.array(image_features)  # Convert to NumPy array\n",
        "print(f\"Total number of image features extracted: {len(image_features)}\")\n",
        "\n",
        "# Tokenize captions\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(captions)\n",
        "captions_sequences = tokenizer.texts_to_sequences(captions)\n",
        "captions_padded = pad_sequences(captions_sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(image_features, captions_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN-LSTM Model\n",
        "def build_model(vocab_size, max_length, dropout=0.5, lstm_units=256):\n",
        "    image_input = Input(shape=(25088,))  # VGG16 output shape\n",
        "    img_dense = Dense(256, activation=\"relu\")(image_input)\n",
        "\n",
        "    text_input = Input(shape=(max_length,))\n",
        "    text_embed = Embedding(vocab_size, 256, mask_zero=True)(text_input)\n",
        "    text_lstm = LSTM(lstm_units, return_sequences=False)(text_embed)\n",
        "\n",
        "    merged = add([img_dense, text_lstm])\n",
        "    merged = Dropout(dropout)(merged)\n",
        "    output = Dense(vocab_size, activation=\"softmax\")(merged)\n",
        "\n",
        "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train Model with K-Fold Cross Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "for train_idx, val_idx in kf.split(X_train):\n",
        "    model = build_model(vocab_size, max_length)\n",
        "\n",
        "    history = model.fit(\n",
        "        [X_train[train_idx], y_train[train_idx]],\n",
        "        y_train[train_idx],\n",
        "        validation_data=([X_train[val_idx], y_train[val_idx]], y_train[val_idx]),\n",
        "        epochs=10, batch_size=64, verbose=1\n",
        "    )\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        os.makedirs(\"model\", exist_ok=True)  # Ensure directory exists\n",
        "        model.save(\"model/best_caption_generator.h5\")\n",
        "\n",
        "print(\"Training complete. Best model saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ED7X-CjkRDb8",
        "outputId": "3baedaf3-d209-44fa-b1bb-0cdd27e42f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "File is not a zip file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c144620efd0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Extract ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temp_images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Images.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temp_images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G1tHIuZGRl-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Load Model & Tokenizer\n",
        "model = load_model(\"model/best_caption_generator.h5\")\n",
        "tokenizer = pickle.load(open(\"data/tokenizer.pkl\", \"rb\"))\n",
        "max_length = 30\n",
        "\n",
        "# Load CNN Model for Feature Extraction\n",
        "vgg = VGG16(weights=\"imagenet\")\n",
        "cnn_model = tf.keras.models.Model(inputs=vgg.input, outputs=vgg.layers[-2].output)\n",
        "\n",
        "def extract_features(img_path):\n",
        "    image = Image.open(img_path).resize((224, 224))\n",
        "    image = np.expand_dims(np.array(image) / 255.0, axis=0)\n",
        "    features = cnn_model.predict(image)\n",
        "    return features.reshape(1, 4096)\n",
        "\n",
        "def generate_caption(img_path):\n",
        "    features = extract_features(img_path)\n",
        "    caption = \"<start>\"\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "        seq = pad_sequences([seq], maxlen=max_length)\n",
        "\n",
        "        y_pred = model.predict([features, seq])\n",
        "        word_id = np.argmax(y_pred)\n",
        "\n",
        "        word = tokenizer.index_word.get(word_id, \"<end>\")\n",
        "        caption += \" \" + word\n",
        "\n",
        "        if word == \"<end>\":\n",
        "            break\n",
        "\n",
        "    return caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
        "\n",
        "# Test Image\n",
        "image_path = \"test_images/sample.jpg\"\n",
        "caption = generate_caption(image_path)\n",
        "print(f\"Generated Caption: {caption}\")\n"
      ],
      "metadata": {
        "id": "pSEeGWoRRZHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load Data\n",
        "actual_captions = pickle.load(open(\"data/actual_captions.pkl\", \"rb\"))\n",
        "predicted_captions = pickle.load(open(\"data/predicted_captions.pkl\", \"rb\"))\n",
        "\n",
        "# BLEU Score Calculation\n",
        "def calculate_bleu(actual, predicted):\n",
        "    return sentence_bleu([actual.split()], predicted.split())\n",
        "\n",
        "# ROUGE Score Calculation\n",
        "def calculate_rouge(actual, predicted):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(predicted, actual)\n",
        "    return scores[0]\n",
        "\n",
        "# Convert Words to Numerical IDs for Precision/Recall/F1\n",
        "def words_to_ids(sentence, tokenizer):\n",
        "    return tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "tokenizer = pickle.load(open(\"data/tokenizer.pkl\", \"rb\"))\n",
        "\n",
        "# Compute Metrics\n",
        "bleu_scores = []\n",
        "rouge_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "for i in range(len(actual_captions)):\n",
        "    bleu = calculate_bleu(actual_captions[i], predicted_captions[i])\n",
        "    rouge = calculate_rouge(actual_captions[i], predicted_captions[i])\n",
        "\n",
        "    actual_ids = words_to_ids(actual_captions[i], tokenizer)\n",
        "    predicted_ids = words_to_ids(predicted_captions[i], tokenizer)\n",
        "\n",
        "    # Ensure lengths match\n",
        "    min_len = min(len(actual_ids), len(predicted_ids))\n",
        "    actual_ids, predicted_ids = actual_ids[:min_len], predicted_ids[:min_len]\n",
        "\n",
        "    precision = precision_score(actual_ids, predicted_ids, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(actual_ids, predicted_ids, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(actual_ids, predicted_ids, average=\"macro\", zero_division=0)\n",
        "\n",
        "    bleu_scores.append(bleu)\n",
        "    rouge_scores.append(rouge[\"rouge-l\"][\"f\"])\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Average BLEU Score: {np.mean(bleu_scores):.4f}\")\n",
        "print(f\"Average ROUGE-L Score: {np.mean(rouge_scores):.4f}\")\n",
        "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
        "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "so_7NZ11RuoC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}