{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "ED7X-CjkRDb8",
        "outputId": "e5fbae4d-ee2a-4be3-e77c-7c2b1ca300ca"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplications\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VGG16\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load Dataset\n",
        "with open(\"captions.txt\", \"r\") as f:\n",
        "    captions = f.readlines()\n",
        "vocab_size = 5000  # Define vocabulary size\n",
        "max_length = 30  # Max caption length\n",
        "\n",
        "with zipfile.ZipFile(\"Images.zip\", \"r\") as zip_ref:\n",
        "    # Extract all image files to a temporary directory\n",
        "    zip_ref.extractall(\"temp_images\")\n",
        "\n",
        "# Process images and extract features using your preferred method (e.g., VGG16)\n",
        "# Here's a placeholder for the process:\n",
        "def extract_features_from_image(image_path):\n",
        "    # Replace this with your actual feature extraction logic\n",
        "    # using libraries like OpenCV or Pillow\n",
        "    # This is just an example, you'll need to adapt it\n",
        "    vgg = VGG16(weights='imagenet', include_top=False)\n",
        "    img = Image.open(image_path).resize((224, 224)) # Use PIL to open and resize the image\n",
        "    img = np.array(img) # Convert the image to a NumPy array\n",
        "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2])) # Reshape the image to (1, height, width, channels)\n",
        "    features = vgg.predict(img, verbose=0)\n",
        "    return features.flatten() # Flatten the features to a 1D array\n",
        "\n",
        "\n",
        "image_features = []\n",
        "import os\n",
        "for filename in os.listdir(\"temp_images\"):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Adjust file extensions as needed\n",
        "        image_path = os.path.join(\"temp_images\", filename)\n",
        "        features = extract_features_from_image(image_path)\n",
        "        image_features.append(features)\n",
        "    print(f\"Processed: {filename}\")\n",
        "\n",
        "image_features = np.array(image_features) # Convert to NumPy array\n",
        "print(f\"Total number of image features extracted: {len(image_features)}\")\n",
        "\n",
        "# The rest of your code can remain the same\n",
        "# ...\n",
        "\n",
        "# Tokenize captions\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(captions)\n",
        "captions_sequences = tokenizer.texts_to_sequences(captions)\n",
        "captions_padded = pad_sequences(captions_sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(image_features, captions_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN-LSTM Model\n",
        "def build_model(vocab_size, max_length, dropout=0.5, lstm_units=256):\n",
        "    image_input = Input(shape=(4096,))\n",
        "    img_dense = Dense(256, activation=\"relu\")(image_input)\n",
        "\n",
        "    text_input = Input(shape=(max_length,))\n",
        "    text_embed = Embedding(vocab_size, 256, mask_zero=True)(text_input)\n",
        "    text_lstm = LSTM(lstm_units, return_sequences=False)(text_embed)\n",
        "\n",
        "    merged = add([img_dense, text_lstm])\n",
        "    merged = Dropout(dropout)(merged)\n",
        "    output = Dense(vocab_size, activation=\"softmax\")(merged)\n",
        "\n",
        "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train Model with K-Fold Cross Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "for train_idx, val_idx in kf.split(X_train):\n",
        "    model = build_model(vocab_size, max_length)\n",
        "\n",
        "    history = model.fit(\n",
        "        [X_train[train_idx], y_train[train_idx]],\n",
        "        y_train[train_idx],\n",
        "        validation_data=([X_train[val_idx], y_train[val_idx]], y_train[val_idx]),\n",
        "        epochs=10, batch_size=64, verbose=1\n",
        "    )\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        model.save(\"model/best_caption_generator.h5\")\n",
        "\n",
        "print(\"Training complete. Best model saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1tHIuZGRl-o"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSEeGWoRRZHt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Load Model & Tokenizer\n",
        "model = load_model(\"model/best_caption_generator.h5\")\n",
        "tokenizer = pickle.load(open(\"data/tokenizer.pkl\", \"rb\"))\n",
        "max_length = 30\n",
        "\n",
        "# Load CNN Model for Feature Extraction\n",
        "vgg = VGG16(weights=\"imagenet\")\n",
        "cnn_model = tf.keras.models.Model(inputs=vgg.input, outputs=vgg.layers[-2].output)\n",
        "\n",
        "def extract_features(img_path):\n",
        "    image = Image.open(img_path).resize((224, 224))\n",
        "    image = np.expand_dims(np.array(image) / 255.0, axis=0)\n",
        "    features = cnn_model.predict(image)\n",
        "    return features.reshape(1, 4096)\n",
        "\n",
        "def generate_caption(img_path):\n",
        "    features = extract_features(img_path)\n",
        "    caption = \"<start>\"\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "        seq = pad_sequences([seq], maxlen=max_length)\n",
        "\n",
        "        y_pred = model.predict([features, seq])\n",
        "        word_id = np.argmax(y_pred)\n",
        "\n",
        "        word = tokenizer.index_word.get(word_id, \"<end>\")\n",
        "        caption += \" \" + word\n",
        "\n",
        "        if word == \"<end>\":\n",
        "            break\n",
        "\n",
        "    return caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
        "\n",
        "# Test Image\n",
        "image_path = \"test_images/sample.jpg\"\n",
        "caption = generate_caption(image_path)\n",
        "print(f\"Generated Caption: {caption}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so_7NZ11RuoC"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load Data\n",
        "actual_captions = pickle.load(open(\"data/actual_captions.pkl\", \"rb\"))\n",
        "predicted_captions = pickle.load(open(\"data/predicted_captions.pkl\", \"rb\"))\n",
        "\n",
        "# BLEU Score Calculation\n",
        "def calculate_bleu(actual, predicted):\n",
        "    return sentence_bleu([actual.split()], predicted.split())\n",
        "\n",
        "# ROUGE Score Calculation\n",
        "def calculate_rouge(actual, predicted):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(predicted, actual)\n",
        "    return scores[0]\n",
        "\n",
        "# Convert Words to Numerical IDs for Precision/Recall/F1\n",
        "def words_to_ids(sentence, tokenizer):\n",
        "    return tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "tokenizer = pickle.load(open(\"data/tokenizer.pkl\", \"rb\"))\n",
        "\n",
        "# Compute Metrics\n",
        "bleu_scores = []\n",
        "rouge_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "for i in range(len(actual_captions)):\n",
        "    bleu = calculate_bleu(actual_captions[i], predicted_captions[i])\n",
        "    rouge = calculate_rouge(actual_captions[i], predicted_captions[i])\n",
        "\n",
        "    actual_ids = words_to_ids(actual_captions[i], tokenizer)\n",
        "    predicted_ids = words_to_ids(predicted_captions[i], tokenizer)\n",
        "\n",
        "    # Ensure lengths match\n",
        "    min_len = min(len(actual_ids), len(predicted_ids))\n",
        "    actual_ids, predicted_ids = actual_ids[:min_len], predicted_ids[:min_len]\n",
        "\n",
        "    precision = precision_score(actual_ids, predicted_ids, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(actual_ids, predicted_ids, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(actual_ids, predicted_ids, average=\"macro\", zero_division=0)\n",
        "\n",
        "    bleu_scores.append(bleu)\n",
        "    rouge_scores.append(rouge[\"rouge-l\"][\"f\"])\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Average BLEU Score: {np.mean(bleu_scores):.4f}\")\n",
        "print(f\"Average ROUGE-L Score: {np.mean(rouge_scores):.4f}\")\n",
        "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
        "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
